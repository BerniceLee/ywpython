{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keras를 이용해서, sequence classification 해보기\n",
    "\n",
    "- 여기서는 imdb에 쌓여 있는 movie review 데이터를 대상으로 sentiment analysis를 수행합니다.\n",
    "    - 단어를 bow, tfidf 를 사용해서 벡터화하거나, n-gram을 활용해서, 문맥을 파악하는 식으로 벡터화하거나, 아무튼 그렇게 해서 classifier를 만들고 적용하는 방식은 이전에 많이 했습니다.\n",
    "- 하지만 여기서는, LSTM을 이용합니다. 뭐, n-gram의 경우처럼 일종의 앞 뒤 문맥을 잘 이용해서 학습한다, 정도로 이해하면 되겠네요.\n",
    "- 전체 뉴럴넷의 아키텍쳐는 다음과 같습니다.\n",
    "    - embedding => convolution => maxpooling => LSTM => Dense\n",
    "    - 중간중간에 dropout은 알아서 잘 넣습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 읽고 padding을 넣어줌\n",
    "- 개별 무비리뷰에는 캐릭터나 스트링이 들어가있는 것이 아니라, word vocabulary의 index가 채워져 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from keras.datasets import imdb\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Conv1D, MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Object arrays cannot be loaded when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-aa87a400de2e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mmax_review_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtop_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ywkeras\\lib\\site-packages\\keras\\datasets\\imdb.py\u001b[0m in \u001b[0;36mload_data\u001b[1;34m(path, num_words, skip_top, maxlen, seed, start_char, oov_char, index_from, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m                     file_hash='599dadb1135973df5b59232a0e9a887c')\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'x_train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y_train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'x_test'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y_test'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ywkeras\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    260\u001b[0m                 return format.read_array(bytes,\n\u001b[0;32m    261\u001b[0m                                          \u001b[0mallow_pickle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m                                          pickle_kwargs=self.pickle_kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ywkeras\\lib\\site-packages\\numpy\\lib\\format.py\u001b[0m in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[0;32m    694\u001b[0m         \u001b[1;31m# The array contained Python objects. We need to unpickle the data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 696\u001b[1;33m             raise ValueError(\"Object arrays cannot be loaded when \"\n\u001b[0m\u001b[0;32m    697\u001b[0m                              \"allow_pickle=False\")\n\u001b[0;32m    698\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpickle_kwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Object arrays cannot be loaded when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "개별 movie review에 있는, 모든 단어를 고려하는 것은 무의미하기 때문에 \n",
    "top_words, 즉 상위 5000개의 단어에 대해서만, 추려냄. 나머지는 필터링\n",
    "그리고 단어는 index로 표시됨.\n",
    "\"\"\"\n",
    "\n",
    "max_review_length, top_words = 100, 500\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# save np.load\n",
    "np_load_old = np.load\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "# call load_data with allow_pickle implicitly set to true\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = top_words)\n",
    "\n",
    "# restore np.load for future normal usage\n",
    "np.load = np_load_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data complete\n"
     ]
    }
   ],
   "source": [
    "X_train = sequence.pad_sequences(X_train, maxlen = max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen = max_review_length)\n",
    "print(\"Read data complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뉴럴넷 아키텍쳐를 만들고, 예측결과를 확인."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layer의 구성은 \n",
    "- Embedding: 단어를 벡터화하고, 이 결과 값을 LSTM에 집어넣어줌. \n",
    "    - input_dim에 top_words를 넣어주는데, 아마도 내부에서 자동으로 one-hot vector를 만들어주는 것 같음\n",
    "    - 현재는 one-hot vector가 아니라, 0, 1, 등 word vocab의 index가 넘어감. \n",
    "- Conv1D: 구조적인 특성을 파악하기 위해 여러 filter로 찍어줌.\n",
    "- MaxPooling1D: convolution으로 찍어낸 정보를 좀 더 특징화함. \n",
    "- LSTM: sequential한 정보를 활용\n",
    "- Dense: classification이므로 output layer을 1칸짜리로 넣어줌. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\ywkeras\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\ywkeras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 32)           16000     \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 100, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 50, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                16600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 39,053\n",
      "Trainable params: 39,053\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\ywkeras\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 22s 866us/step - loss: 0.5306 - acc: 0.7188 - val_loss: 0.4294 - val_acc: 0.8036\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 20s 783us/step - loss: 0.4177 - acc: 0.8108 - val_loss: 0.4041 - val_acc: 0.8156\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 21s 857us/step - loss: 0.3961 - acc: 0.8210 - val_loss: 0.3935 - val_acc: 0.8215\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 23s 930us/step - loss: 0.3813 - acc: 0.8289 - val_loss: 0.3877 - val_acc: 0.8206\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 24s 963us/step - loss: 0.3670 - acc: 0.8383 - val_loss: 0.3836 - val_acc: 0.8248\n",
      "training complete\n",
      "Accuracy: 82.48\n"
     ]
    }
   ],
   "source": [
    "embedding_vector_length = 32\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=top_words, # 500\n",
    "              output_dim=embedding_vector_length, # 32\n",
    "              input_length=max_review_length), \n",
    "    Conv1D(filters=32, kernel_size=5, padding='same', activation='relu'), \n",
    "    MaxPooling1D(pool_size=2),\n",
    "    LSTM(50), # 원래는 100, \n",
    "    Dropout(0.2), \n",
    "    Dense(25, activation='sigmoid'), \n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=64)\n",
    "print(\"training complete\")\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: {:.2f}\".format(scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras2",
   "language": "python",
   "name": "ywkeras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
