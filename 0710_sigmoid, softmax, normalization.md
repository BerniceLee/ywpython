앞서 리뷰 파일에서는 케라스를 사용하지 않은 코드고, 강사님은 케라스를 쓰기 때문에 케라스 코드로 재리뷰.


## Sigmoid


일단 시그모이드는 이진분류니까 loss(cost)는 binary_crossentropy 로 설정한다.


```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras.optimizers import RMSprop # 아직 안쓸거임
import numpy as np
from tensorflow.keras.optimizers import SGD

x_data = np.array([2,4,5,7,8,10])
y_data = np.array([0,0,0,1,1,1])

model = Sequential()
# Dense - 1개의 완전연결 unit 을 가지고, 차원수는 1
model.add(Dense(1, input_dim=1))
model.add(Activation('sigmoid'))

# SGD : 확률적 경사 하강법
# 랜덤하게 추출한 일부 데이터에 대해 가중치를 조절시켜줌
sgd = SGD(lr=0.00001)
model.compile(loss='binary_crossentropy', optimizer=sgd)
model.fit(x_data, y_data, epochs=1000)

h = model.predict(np.array(9))

model.predict_clases(np.array(9))
```

내가 케라스가 어렵게 느껴지고 직관적이지 않았던게, 케라스에서 쓰는 메소드들이 뭘 의미하는지 잘 몰라서였음

그래서 정리해보면...

- Dense : Dense(1) 이면 1개의 완전연결 unit 을 가지고 있다는 거임.
- Sequential() : 순차적이라는 뜻. 모델을 만들때 처음에 Sequential 로 초기화 한다.
- SGD : 확률적 경사 하강법.
  - 랜덤하게 추출한 일부 데이터에 대해 가중치를 조절시켜줌
  
  
위 모델 학습의 출력값은 다음과 같다.


```python
'''
Epoch 1/1000
6/6 [==============================] - 0s 61ms/sample - loss: 1.5940
Epoch 2/1000
6/6 [==============================] - 0s 326us/sample - loss: 1.5939
Epoch 3/1000
6/6 [==============================] - 0s 333us/sample - loss: 1.5938
Epoch 4/1000
6/6 [==============================] - 0s 332us/sample - loss: 1.5936
Epoch 5/1000
6/6 [==============================] - 0s 332us/sample - loss: 1.5935
Epoch 6/1000
6/6 [==============================] - 0s 159us/sample - loss: 1.5934
  ...
Epoch 995/1000
6/6 [==============================] - 0s 332us/sample - loss: 1.4703
Epoch 996/1000
6/6 [==============================] - 0s 333us/sample - loss: 1.4702
Epoch 997/1000
6/6 [==============================] - 0s 332us/sample - loss: 1.4701
Epoch 998/1000
6/6 [==============================] - 0s 332us/sample - loss: 1.4700
Epoch 999/1000
6/6 [==============================] - 0s 332us/sample - loss: 1.4699
Epoch 1000/1000
6/6 [==============================] - 0s 333us/sample - loss: 1.4697
```


코드 초반부에 set_random_seed() 달아주자.


> set_random_seed 로 난수픽을 잡아줘도 SGD가 랜덤으로 샘플을 추출하는 경사하강이니까 결국 무쓸모 아닐까?


이걸 질문하니 다음과 같은 답변을 해주심

- SGD가 EGD랑 다르게, 랜덤으로 샘플을 추출해서 쓰는 경사하강인건 맞는데,
- SGD 또한 numpy의 난수를 참조를 한다.
- 따라서, set_random_seed 로 **numpy의 난수** 값을 잡아주면,
- SGD 에서 랜덤으로 뽑아내는 샘플값의 "절댓값" 도 동일한 값으로 추출되게 된다.
- 따라서 seed 를 잡아주면 몇 번을 실행해도 추출하는 샘플값은 고정이 된다.



## Softmax



Multinomial Regression 이라서, cost 는 categorical_crossentropy 로 적용시킴


```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
import numpy as np

x_data = np.array([[1, 2, 1, 1],
                   [2, 1, 3, 2],
                   [3, 1, 3, 4],
                   [4, 1, 5, 5],
                   [1, 7, 5, 5],
                   [1, 2, 5, 6],
                   [1, 6, 6, 6],
                   [1, 7, 7, 7]],
                  dtype=np.float32)

y_data = np.array([[0, 0, 1],
                   [0, 0, 1],
                   [0, 0, 1],
                   [0, 1, 0],
                   [0, 1, 0],
                   [0, 1, 0],
                   [1, 0, 0],
                   [1, 0, 0]],
                  dtype=np.float32)

model = Sequential()
# (4, 뒷부분은 정해지지 않았으니 남겨둔다. (혹은 None 으로 설정))
model.add(Dense(3, input_shape=(4,)))
model.add(Activation('softmax'))

model.summary()

model.compile(loss='categorical_crossentropy',
              optimizer='sgd',
              metrics=['accuracy'])

model.fit(x_data, y_data, epochs=1000)

print("-" * 50)
print(model.predict_classes(np.array([[1, 2, 1, 1]])))
print(model.predict_classes(np.array([[1, 2, 5, 6]])))
```

```python
'''
Model: "sequential_3"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_3 (Dense)              (None, 3)                 15        
_________________________________________________________________
activation_3 (Activation)    (None, 3)                 0         
=================================================================
Total params: 15
Trainable params: 15
Non-trainable params: 0
_________________________________________________________________
Epoch 1/1000
8/8 [==============================] - 0s 4ms/sample - loss: 3.6433 - acc: 0.3750
Epoch 2/1000
8/8 [==============================] - 0s 373us/sample - loss: 3.1519 - acc: 0.3750
Epoch 3/1000
8/8 [==============================] - 0s 249us/sample - loss: 2.6873 - acc: 0.3750
Epoch 4/1000
8/8 [==============================] - 0s 249us/sample - loss: 2.2713 - acc: 0.3750
Epoch 5/1000
8/8 [==============================] - 0s 248us/sample - loss: 1.9299 - acc: 0.3750
Epoch 6/1000
8/8 [==============================] - 0s 249us/sample - loss: 1.6750 - acc: 0.5000
Epoch 7/1000
8/8 [==============================] - 0s 376us/sample - loss: 1.4933 - acc: 0.5000
  ...
Epoch 995/1000
8/8 [==============================] - 0s 249us/sample - loss: 0.5457 - acc: 0.8750
Epoch 996/1000
8/8 [==============================] - 0s 249us/sample - loss: 0.5456 - acc: 0.8750
Epoch 997/1000
8/8 [==============================] - 0s 249us/sample - loss: 0.5455 - acc: 0.8750
Epoch 998/1000
8/8 [==============================] - 0s 249us/sample - loss: 0.5454 - acc: 0.8750
Epoch 999/1000
8/8 [==============================] - 0s 125us/sample - loss: 0.5453 - acc: 0.8750
Epoch 1000/1000
8/8 [==============================] - 0s 249us/sample - loss: 0.5452 - acc: 0.8750
--------------------------------------------------

[2]
[1]

```

Prediction 값은 각 array 샘플마다 2와 1이 출력된다. (잘 예측된거임)


> 왜 2랑 1이 예측된건지?

- 샘플은 각각 1번째, 6번째 샘플을 차용한건데
- 1번째 샘플의 one-hot-encoding 된 정답값은 0,0,1 이고, 정답인 1의 인덱스 값은 2
- 마찬가지로 6번째 샘플의 one-hot-encoding 된 정답값은 0,1,0 이고, 정답인 1의 인덱스 값은 1



## 데이터 전처리를 위한 개념


실제 데이터로 쓰려면 우선 데이터 전처리 과정이 필요하다.

데이터 전처리 과정에 쓰이는 정규화에 대해서 다뤄보고자 함.


### 정규화 (Normalization)


우선 정규화가 필요한 이유부터 살펴보자.

> 결론부터 말하면, Overfitting 과 Underfitting 을 피하기 위함.


실제 데이터에선 feature의 범위가 매우 클 수 있다. 범위가 매우 크기 때문에 **feature 의 범위를 보정해주는 과정**이 필요하다.

이 과정을 정규화 라고 한다.


![Normalization](https://t1.daumcdn.net/cfile/tistory/99E0C33359C1476F0B)


> 정규화된 값 = (원본값 - feature의 최소값) / (feature의 최대값 - feature의 최소값) 


정규화의 핵심은, **최소값과 최대값**을 이용한다는 것이다.


### 손실함수, 비용함수, 목적함수


모델을 학습할 때는 비용(cost) 즉, 오류를 최소화하는 방향으로 진행이 된다.

비용이 최소화되는 곳이 성능이 가장 잘 나오는 부분이며,
가능한 비용이 적은 부분을 찾는 것이 **최적화(Optimization)**이고,
**일반화(Generalization)**의 방법이라고 할 수 있다.

이 비용(cost) 혹은 손실(loss)이 얼마나 있는지 나타내는 것이
**비용 함수(Cost Function)**, **손실 함수(Loss Function)**이라고 할 수 있다.


> 손실함수 vs 비용함수 vs 목적함수


1. 손실함수

![loss function](https://postfiles.pstatic.net/MjAxODEwMjdfMjQ4/MDAxNTQwNjQxMDM2Nzk2.BBIktuYfZGDOltDPfnFZtGtGyAytqArZhHrc2uh2vCMg.Amk9px-LZyU2cH9WTmHwv_NA38VsVtgtV49FRbhRnX0g.PNG.qbxlvnf11/20181027_205024.png?type=w773)


위 정의에서 보면, **single data set** 을 다룬다.

즉, 특정 값과 파라미터를 설정해서 어떤 (single) 결과값이 나옴.


2. 비용함수

![cost function](https://postfiles.pstatic.net/MjAxODEwMjdfMTI3/MDAxNTQwNjQxMzQ3NDIy.Ud2tujcppuIOyhBsUgdf1X3eu9Ea-6K2zZfdqONRLiAg.RcUTI8Da51xFVHWXMsJsEjAJ_tTo6IaFoddR4MACVIgg.PNG.qbxlvnf11/20181027_205501.png?type=w773)ㅣ


간단하게 말하면, **loss function의 합** 이다.

즉, single data set이 아니라 **entire data set**을 다룬다.


3. 목적함수

![organized function](https://postfiles.pstatic.net/MjAxODEwMjdfMjM1/MDAxNTQwNjQxMzQ3NDU2.cpPY6GJ0oeJLiz4h_Xs3pKVuv5V5f32aHSfbfX_rsEgg.a2QE606kUdtjPVt2JBlbpB4ZtgQST06b1dZN9KqOnSkg.PNG.qbxlvnf11/20181027_205532.png?type=w773)


모델에 우리가 가장 일반적으로 사용하는 용어.

즉, **최댓값, 최솟값을 구하는 함수** 를 말함.


### 실제 예제를 통한 데이터 분리 및 전처리


이 쯤에서 예제를 하나 돌려볼건데,

데이터 분리를 하기 위해서 **hold-out-validation** 을 쓸 것임.

- Training / Validation 으로 쪼개는 것을 말함.


#### Boston Housing


보스턴의 주택 가격 데이터를 통해서 주택 가격을 예측해보자.


scikit-learn 이 제공하는 예제 데이터의 속성은 다음과 같다.

- 타겟 데이터
  - 1978 보스턴 주택 가격
  - 506개 타운의 주택 가격 중앙값 (단위 1,000 달러)
- 특징 데이터
  - CRIM: 범죄율
  - INDUS: 비소매상업지역 면적 비율
  - NOX: 일산화질소 농도
  - RM: 주택당 방 수
  - LSTAT: 인구 중 하위 계층 비율
  - B: 인구 중 흑인 비율
  - PTRATIO: 학생/교사 비율
  - ZN: 25,000 평방피트를 초과 거주지역 비율
  - CHAS: 찰스강의 경계에 위치한 경우는 1, 아니면 0
  - AGE: 1940년 이전에 건축된 주택의 비율
  - RAD: 방사형 고속도로까지의 거리
  - DIS: 직업센터의 거리
  - TAX: 재산세율
  
```python
load_boston()
```

명령어를 통해서 데이터를 로드하고, **DESCR** 속성으로 문서 설명을 열람할 수 있다.

예제코드를 살펴보자.


```python
# 일부만 할당된 GPU 메모리를 실행하는 동안 필요한만큼 늘릴 수 있도록 설정함
import tensorflow as tf

config = tf.ConfigProto()
config.gpu_options.allow_growth = True
session = tf.Session(config=config)

# 동일한 결과를 재현하기 위해서 random seed 고정
import numpy as np

np.random.seed(777)

# 모델 학습 시 불필요한 출력을 끄도록 함 (warning)
import tensorflow as tf

tf.logging.set_verbosity(tf.logging.ERROR)
```


**1. 데이터 준비**


```python
from tensorflow.keras.datasets import boston_housing

(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()
```

이거 하면 

```python
'''
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz
57344/57026 [==============================] - 0s 1us/step
'''
```

알아서 잘 다운받아짐.


```python

print(train_data.shape)
print(test_data.shape)

'''
(404, 13)
(102, 13)
'''
```

shape 를 뽑아보면, train_data는 갯수 404개, 특성은 13개

test_data는 갯수 102개, 특성은 13개로 확인 가능함.


```python
print(train_targets) # 단위: 천 달러
```


```python
'''
[15.2 42.3 50.  21.1 17.7 18.5 11.3 15.6 15.6 14.4 12.1 17.9 23.1 19.9
 15.7  8.8 50.  22.5 24.1 27.5 10.9 30.8 32.9 24.  18.5 13.3 22.9 34.7
 16.6 17.5 22.3 16.1 14.9 23.1 34.9 25.  13.9 13.1 20.4 20.  15.2 24.7
 22.2 16.7 12.7 15.6 18.4 21.  30.1 15.1 18.7  9.6 31.5 24.8 19.1 22.
 14.5 11.  32.  29.4 20.3 24.4 14.6 19.5 14.1 14.3 15.6 10.5  6.3 19.3
 19.3 13.4 36.4 17.8 13.5 16.5  8.3 14.3 16.  13.4 28.6 43.5 20.2 22.
 23.  20.7 12.5 48.5 14.6 13.4 23.7 50.  21.7 39.8 38.7 22.2 34.9 22.5
 31.1 28.7 46.  41.7 21.  26.6 15.  24.4 13.3 21.2 11.7 21.7 19.4 50.
 22.8 19.7 24.7 36.2 14.2 18.9 18.3 20.6 24.6 18.2  8.7 44.  10.4 13.2
 21.2 37.  30.7 22.9 20.  19.3 31.7 32.  23.1 18.8 10.9 50.  19.6  5.
 14.4 19.8 13.8 19.6 23.9 24.5 25.  19.9 17.2 24.6 13.5 26.6 21.4 11.9
 22.6 19.6  8.5 23.7 23.1 22.4 20.5 23.6 18.4 35.2 23.1 27.9 20.6 23.7
 28.  13.6 27.1 23.6 20.6 18.2 21.7 17.1  8.4 25.3 13.8 22.2 18.4 20.7
 31.6 30.5 20.3  8.8 19.2 19.4 23.1 23.  14.8 48.8 22.6 33.4 21.1 13.6
 32.2 13.1 23.4 18.9 23.9 11.8 23.3 22.8 19.6 16.7 13.4 22.2 20.4 21.8
 26.4 14.9 24.1 23.8 12.3 29.1 21.  19.5 23.3 23.8 17.8 11.5 21.7 19.9
 25.  33.4 28.5 21.4 24.3 27.5 33.1 16.2 23.3 48.3 22.9 22.8 13.1 12.7
 22.6 15.  15.3 10.5 24.  18.5 21.7 19.5 33.2 23.2  5.  19.1 12.7 22.3
 10.2 13.9 16.3 17.  20.1 29.9 17.2 37.3 45.4 17.8 23.2 29.  22.  18.
 17.4 34.6 20.1 25.  15.6 24.8 28.2 21.2 21.4 23.8 31.  26.2 17.4 37.9
 17.5 20.   8.3 23.9  8.4 13.8  7.2 11.7 17.1 21.6 50.  16.1 20.4 20.6
 21.4 20.6 36.5  8.5 24.8 10.8 21.9 17.3 18.9 36.2 14.9 18.2 33.3 21.8
 19.7 31.6 24.8 19.4 22.8  7.5 44.8 16.8 18.7 50.  50.  19.5 20.1 50.
 17.2 20.8 19.3 41.3 20.4 20.5 13.8 16.5 23.9 20.6 31.5 23.3 16.8 14.
 33.8 36.1 12.8 18.3 18.7 19.1 29.  30.1 50.  50.  22.  11.9 37.6 50.
 22.7 20.8 23.5 27.9 50.  19.3 23.9 22.6 15.2 21.7 19.2 43.8 20.3 33.2
 19.9 22.5 32.7 22.  17.1 19.  15.  16.1 25.1 23.7 28.7 37.2 22.6 16.4
 25.  29.8 22.1 17.4 18.1 30.3 17.5 24.7 12.6 26.5 28.7 13.3 10.4 24.4
 23.  20.  17.8  7.  11.8 24.4 13.8 19.4 25.2 19.4 19.4 29.1]
'''
```


**2. 데이터 전처리**


```python
mean = train_data.mean(axis=0)
train_data -= mean

std = train_data.std(axis=0)
train_data /= std

# 테스트 셋도 훈련 셋에서 계산된 mean과 std를 동일하게 사용함
# 만약 각각 정규화를 수행해버리면, 학습 결과 적용이 불가능해짐
test_data -= mean
test_data /= std
```

13가지 특성에 대한 평균을 구하고 그 평균값을 빼준 후에

각 값을 제곱하여 더하고 배열 길이로 나눠준 후 (분산) 루트√를 씌워 표준편차를 구한다.

그 표준편차로 다시 13가지 특성에서 평균값을 빼준 값을 나눈다.

test_data 또한 같은 과정을 거치는데 주의할 점은 이미 구한 훈련데이터의 평균과
표준편차를 이용하여야 한다는 것이다.

(평균이 m이고, 표준편차가 3이라고 할때, 실제 값은 m+-3)



**3. 네트워크 정의**



```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

def build_model():
    model = Sequential()
    model.add(Dense(64, activation='relu', input_shape=(train_data.shape[1],)))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(1)) # 회귀 모델 (출력층에 활성화 함수 없음)
    
    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])
    
    return model
```


여기서 activation 은 렐루를 쓸건데, 렐루가 뭔지 잠깐만 살펴보자.


#### ReLU 함수


기존에 학습했던 시그모이드는 Vanisning Gradient 라는 심각한 문제가 발생한다.

렐루함수는 이를 보완하기 위해 나옴.


![sigmoid vs. relu](https://t1.daumcdn.net/cfile/tistory/22293C50579F7BBF13)


> 그냥 지금 간단하게 말하면, 음수값에 대해 0으로 처리하는 것을 의미한다.


간단하게 렐루 함수를 짚어봤고 다시 코드를 보면,

모델을 컴파일할 때, RMSProp 을 이용해준다. 이게 뭔지만 마찬가지로 짚고 넘어간다.


#### RMSProp


RMSProp 을 이해하려면, 아다그라드를 먼저 이해해야하는데,

> Adagrad (Adaptive Gradient) : 변수의 업데이트 횟수에 따라 학습률(Learning rate)를 조절하는 옵션이 추가된 최적화 방법

아다그라드는 많이 변화하지 않은 변수들은 학습률(step size)를 크게하고, 
반대로 많이 변화한 변수들에 대해서는 학습률을 적게한다.

이는 많이 변화한 변수는 최적값에 근접했을 것이라는 가정하에 작은 크기로 이동하면서 세밀한 값을 조정하고, 

반대로 적게 변화한 변수들은 학습률을 크게하여 빠르게 loss값을 줄임.


**RMSProp**는, 아다그라드의 G(t)의 값이 무한히 커지는 것을 방지하고자 제안된 방법

> RMSProp 은 "지수 이동 평균" 이다.


지수 이동 평균은, 쉽게 말해 최근 값을 더 잘 반영하기 위해 최근 값에 값과 이전 값에 각각 가중치를 주어 계산하는 방법이다.


![RMSProp](https://t1.daumcdn.net/cfile/tistory/99AC26425B16298C09)


위 식에서 
- 지수 이동편균값은 x, 현재 값은 p, 가중치는 α(알파)이며, 
- 아래 첨자 k는 step 또는 시간, 마지막으로 N은 값의 개수

만약 처음부터 현재까지 계산을 하게 되면, N과 k의 값은 같고, 가중치 알파는 N이 작을 수록 커지게 됨.


> 즉, 1주기가 지날 때마다 (1-α)라는 가중치가 이전 값에 곱해지는데, (1-α) 값이 1보다 작기 때문에 시간이 지날수록 영향력이 줄어듦



**4. (K-겹 교차 검증을 사용한) 모델 학습**



위에서 언급한 K-fold validation 을 써서 모델을 학습시켜볼거임

여기서 k는 4 정도로 두고, epoch는 100 정도로 두고 한번 돌려보자.


```python
import numpy as np

k = 4
num_val_samples = len(train_data) // k
num_epochs = 100
all_scores = []

for i in range(k):
    print('>> fold ', i)
    # 검증 데이터 준비: k번째 분할
    val_data = train_data[i * num_val_samples: (i+1) * num_val_samples]
    val_targets = train_targets[i * num_val_samples: (i+1) * num_val_samples]
    
    # 훈련 데이터 준비: 다른 분할 전체
    partial_train_data = np.concatenate([train_data[:i*num_val_samples],
                                         train_data[(i+1) * num_val_samples:]],
                                        axis=0)
    partial_train_targets = np.concatenate([train_targets[:i*num_val_samples],
                                            train_targets[(i+1) * num_val_samples:]],
                                           axis=0)
                                           
    # 케라스 모델 구성(컴파일 포함)
    model = build_model()
    
    # 모델 훈련(verbose=0 이므로 훈련 과정이 출력되지 않습니다)
    model.fit(partial_train_data, 
              partial_train_targets,
              epochs = num_epochs,
              batch_size = 1,
              verbose = 0)
              
    # 검증 세트로 모델 평가
    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)
    all_scores.append(val_mae)

print('finished!!')
```

```python
'''
>> fold  0
>> fold  1
>> fold  2
>> fold  3
finished!!
'''
```


일단 저 코드를 이해하려면 MAE랑 verbose 가 뭔지 먼저 알아야함.



#### MAE (Mean Absolute Error)


![MAE](https://i.imgur.com/tqnei6J.jpg)


모델의 예측값과 실제값의 차이를 모두 더한다는 개념이다.


- 절대값을 취하기 때문에 가장 직관적으로 알 수 있는 지표
- MSE 보다 특이치에 robust함
- 절대값을 취하기 때문에 모델이 underperformance 인지 overperformance 인지 알 수 없음


#### verbose


model.fit documentation 을 참고하면..

- verbose=0 : 아무것도 안보여 준다는 뜻 (silent)
- verbose=1 : ============ 이런 progress bar 를 출력한다는 뜻
- verbose=2 : Epoch num 만 출력한다는 뜻 (Epoch 1/10)



그래서, 위 코드를 왜 저렇게 접근했는지 살펴보면....

- 샘플수가 404개.. 적다. 그대로 실행하면 신뢰 있는 모델 평가를 할 수가 없음.
- 그래서 K-fold 로 여러겹으로 나눠서 여러차례 실행해줌
- k=4니까 4번 실행, 각 fold 마다 100회씩 훈련해서 총 400회 훈련하게 됨.

우선, 각 K 조각마다 검증 데이터의 mae 를 구하고 평균값을 낸다

전체 MAE 값을 한번 뽑아보면....


```python
print(all_mae_histories)
```

```python
'''

[[4.6104016, 3.4632838, 2.824047, 2.8458567, 2.4269226, 2.2648325, 2.3008049, 2.2784483, 2.1543944, 2.1961174, 2.2528293, 2.2098713, 2.0220335, 2.3499138, 2.2977564, 1.9395485, 1.989901, 1.906563, 2.09499, 2.2036355, 2.1482472, 2.1611097, 1.9980994, 2.1118066, 1.8897011, 1.9851369, 2.0258305, 2.088579, 1.8626087, 1.7571441, 2.1656344, 1.8498253, 2.0142746, 2.1878672, 2.0275097, 1.8577193, 2.0782764, 2.0773036, 1.7481501, 2.0537739, 2.0622647, 2.0491416, 2.2230213, 1.9057487, 1.9472346, 1.9096178, 2.2303712, 2.1304145, 1.9353513, 2.064103, 1.9068844, 2.006572, 2.150982, 1.7535794, 2.2820776, 2.1822958, 1.8861543, 1.9898361, 1.9778409, 1.926214, 2.0823545, 2.139705, 2.0955598, 2.1283324, 1.9180698, 1.9344147, 2.0730789, 1.9557921, 2.3136833, 2.1290786, 2.019083, 1.872533, 1.8913206, 2.377101, 2.1335084, 2.3428314, 2.011732, 2.4944324, 2.6371553, 2.2513797, 2.2121425, 2.0592327, 2.1914978, 1.8835539, 2.067749, 2.2140968, 2.155371, 2.1936395, 2.1229224, 2.1564121, 2.4526784, 2.1299803, 2.0646875, 2.1169443, 1.9960095, 2.2086232, 1.9925796, 2.0799308, 2.112436, 2.1633704, 2.0985062, 2.1415172, 1.958134, 2.1337688, 2.2279038, 2.305395, 2.1736639, 2.2901726, 2.197354, 2.074344, 2.2206283, 2.0662577, 2.1519809, 1.9994298, 2.157659, 2.4785817, 2.6603796, 1.9944476, 2.0327442, 2.0228515, 2.1740441, 2.3450341, 2.1253614, 2.290358, 2.2466636, 2.2248447, 2.3046432, 2.115819, 2.4294143, 2.0192077, 2.2616138, 2.2807212, 2.2824707, 2.2063231, 2.235564, 2.443719, 2.2138362, 2.1850703, 2.3002644, 2.1175597, 2.4564009, 2.2272646, 2.10759, 2.312762, 2.0621274, 2.2347174, 2.2167552, 2.0480547, 2.1502774, 2.4851434, 2.0639067, 2.2407835, 2.4677672, 2.2856815, 2.3779745, 2.0083141, 2.506184, 2.1308022, 2.3451602, 2.2893302, 2.4992125, 2.49442, 2.2013838, 2.2701364, 2.2098057, 2.4278092, 2.230765, 2.1678915, 2.2369862, 2.2352, 2.1041534, 2.303779, 2.3379767, 2.5861, 2.3849068, 2.1463802, 2.3708346, 2.373013, 2.6164174, 2.1640282, 2.2359712, 2.3593748, 2.1711555, 2.3865092, 2.2482455, 2.3774137, 2.3475213, 2.1352623, 2.2117944, 2.4048047, 2.2637277, 2.261944, 2.2460837, 2.3576744, 2.370865, 2.142941, 2.3436117, 2.2284305, 2.4026473, 2.3064759, 2.2096212, 2.1726336, 2.3678286, 2.445004, 2.3550172, 2.2559357, 2.3906558, 2.4016378, 2.303795, 2.4928615, 2.283217, 2.3637729, 2.2557778, 2.3809686, 2.26924, 2.3509555, 2.3849425, 2.2656162, 2.2785008, 2.3427353, 2.318214, 2.400402, 2.3309689, 2.3266637, 2.313617, 2.29627, 2.3944843, 2.3106287, 2.4313834, 2.3312988, 2.3164175, 2.2179828, 2.422175, 2.3299568, 2.3859172, 2.3646202, 2.5166397, 2.5091257, 2.3270235, 2.4615996, 2.370496, 2.3369799, 2.3164659, 2.2050908, 2.6297736, 2.5481677, 2.474224, 2.3818946, 2.3369715, 2.1805491, 2.931272, 2.4260087, 2.2798517, 2.367603, 2.4171402, 2.459235, 2.2274804, 2.5710328, 2.3635366, 2.19659, 2.4004028, 2.3674257, 2.3503683, 2.259389, 2.308265, 2.2887566, 2.285669, 2.4256008, 2.4298398, 2.4109335, 2.399719, 2.3475828, 2.2720366, 2.277654, 2.4665656, 2.258115, 2.213277, 2.414781, 2.3965228, 2.142887, 2.287393, 2.246424, 2.234272, 2.2977479, 2.463273, 2.389726, 2.4561703, 2.3983235, 2.5544205, 2.3718903, 2.3129644, 2.487095, 2.2562308, 2.21268, 2.4096208, 2.1558955, 2.4816859, 2.4177074, 2.3867633, 2.304313, 2.4253936, 2.2037475, 2.2435293, 2.37386, 2.2130766, 2.3607605, 2.2205255, 2.4414086, 2.4670794, 2.5388813, 2.3389618, 2.31344, 2.2852108, 2.501348, 2.3045528, 2.2721295, 2.3473365, 2.267054, 2.2666698, 2.3949256, 2.5413148, 2.4738746, 2.4919848, 2.252986, 2.319347, 2.4448783, 2.4476967, 2.227195, 2.3828528, 2.2905216, 2.3260772, 2.272497, 2.4686074, 2.296876, 2.3709304, 2.7317863, 2.49246, 2.2559981, 2.3907561, 2.504674, 2.4155622, 2.387373, 2.5080569, 2.4872081, 2.3259358, 2.646123, 2.3794794, 2.2680235, 2.6470695, 2.4152372, 2.3869865, 2.3254685, 2.562616, 2.6657631, 2.5265138, 2.3915265, 2.2282114, 2.452877, 2.645536, 2.4780712, 2.5565155, 2.5017004, 2.3474882, 2.4799683, 2.3546004, 2.459985, 2.321158, 2.3505404, 2.4705956, 2.3477554, 2.4361243, 2.4226918, 2.404194, 2.355838, 2.4214528, 2.3777869, 2.4811025, 2.7581208, 2.5622602, 2.4260206, 2.404558, 2.405089, 2.3562772, 2.5927923, 2.3799589, 2.3967414, 2.345935, 2.4623256, 2.3090975, 2.2542133, 2.5605054, 2.44165, 2.238189, 2.28962, 2.3868325, 2.4202816, 2.2173324, 2.5278156, 2.4238577, 2.2841113, 2.2054112, 2.382966, 2.2785957, 2.546733, 2.370772, 2.451194, 2.4541304, 2.344523, 2.5045469, 2.6296287, 2.3906257, 2.4333048, 2.2472887, 2.42097, 2.4207287, 2.5086317, 2.3752122, 2.438632, 2.3632193, 2.2842038, 2.5365157, 2.2901387, 2.2540839, 2.2070234, 2.2597456, 2.3491724, 2.4042253, 2.3919837, 2.387524, 2.3830972, 2.2998672, 2.2566679, 2.5380464, 2.331002, 2.3593192, 2.2653162, 2.444214, 2.4889789, 2.348692, 2.480288, 2.3182635, 2.295481, 2.231466, 2.1749415, 2.4004376, 2.4280019, 2.3995008, 2.3180199, 2.2614412, 2.3536468, 2.3760319, 2.2572823, 2.2524903, 2.2226496, 2.6568425, 2.2729976, 2.2911859, 2.427784, 2.4575999, 2.566887, 2.4371815, 2.459193, 2.5120678, 2.425296, 2.337153, 2.5079663, 2.3758705, 2.4178865, 2.2778733, 2.4548285, 2.454805, 2.2440977, 2.4524515, 2.2789564, 2.27259, 2.3764367, 2.4922862, 2.400237, 2.5406313, 2.2918477, 2.4915752, 2.586506, 2.5199008, 2.3582637, 2.468621, 2.4909701, 2.3852298, 2.4602132, 2.4548411, 2.373895, 2.3156636, 2.226397, 2.3357286, 2.3345006, 2.3136604, 2.412646, 2.2334526, 2.451552, 2.4833248, 2.470347], [4.8001547, 3.3236287, 3.1244042, 2.8966532, 2.8825943, 2.756334, 2.7326279, 2.8455656, 2.7004144, 2.5863953, 2.674301, 2.6253908, 2.6948004, 2.7994194, 2.7671146, 2.4570017, 2.5971446, 2.4089928, 2.7340431, 2.5446575, 2.4289842, 2.601852, 2.5835416, 2.681971, 2.5009198, 2.6616905, 2.5328135, 2.441957, 2.3654225, 2.4632182, 2.4360235, 2.3766203, 2.3554955, 2.5870488, 2.6299536, 2.7930853, 2.516824, 2.4868639, 2.5218894, 2.5418282, 2.350578, 2.5077858, 2.4056609, 2.374167, 2.3417792, 2.3778296, 2.3566408, 2.2716746, 2.4960425, 2.4525056, 2.2943134, 2.6157014, 2.593402, 2.333684, 2.823303, 2.3795347, 2.8650188, 2.4713185, 2.3419063, 2.5320547, 2.363399, 2.1335893, 2.171082, 2.308786, 2.2672296, 2.4738903, 2.267678, 2.3122594, 2.332578, 2.665171, 2.3169377, 2.478989, 2.4485996, 2.700085, 2.3162014, 2.2977562, 2.292593, 2.3114903, 2.6143038, 2.3081007, 3.297243, 2.4775014, 2.3295703, 2.0929518, 2.3010247, 2.5300725, 2.244279, 2.3274403, 2.261891, 2.4061768, 2.2555134, 2.511918, 2.1625836, 2.347305, 2.561399, 2.2944956, 2.4500918, 2.5881667, 3.0403986, 2.9522548, 2.614053, 2.4203417, 2.450299, 2.5095298, 2.5365915, 2.5735645, 2.556982, 2.7794192, 2.7591608, 2.6733592, 2.2251058, 2.622745, 2.3565364, 2.3555655, 2.4813855, 2.328596, 2.3484297, 2.5492227, 2.3584874, 2.2868621, 2.5931973, 2.3767822, 2.2837353, 2.4165437, 2.3980699, 2.3974595, 2.436994, 2.7107077, 2.5040615, 2.6823323, 2.5213203, 2.5862648, 2.5799706, 2.6098511, 2.6186342, 2.6128213, 2.9249268, 3.057658, 2.6741602, 2.919802, 2.7440705, 2.5064576, 2.4846518, 2.72428, 2.8996778, 2.8442912, 2.4829714, 2.5324836, 2.9314375, 2.6344457, 2.4466608, 2.4796016, 2.4717684, 2.6752446, 2.5849483, 2.9303284, 2.6864948, 2.8788688, 2.6520455, 3.2495995, 2.4750698, 2.5471385, 2.753724, 2.8185687, 3.1135592, 2.5885882, 2.6824868, 2.92203, 2.8289561, 2.690033, 2.5791693, 2.6354587, 2.5298436, 2.6456897, 2.6969934, 2.8247104, 2.9147508, 2.747148, 3.1459563, 2.783344, 3.1202753, 2.8082945, 2.8048682, 2.6048656, 3.1469915, 2.4302628, 2.7786694, 2.5664213, 2.619563, 2.56811, 2.6701472, 2.9589415, 2.4929435, 3.1016533, 2.6986287, 2.845679, 2.8181958, 3.5463731, 2.7102256, 2.9158678, 2.611534, 2.6128857, 2.3951955, 2.3782248, 2.7388508, 2.7826135, 2.6490757, 2.7700176, 2.7247717, 3.0182838, 3.034939, 2.6491363, 2.4256513, 2.8360517, 2.5564249, 2.857719, 2.7704782, 2.544428, 3.105485, 2.9948757, 2.647932, 2.8238766, 2.576587, 3.0526521, 2.9107301, 2.7369962, 2.9053311, 2.580711, 2.6155577, 2.5459828, 2.849829, 2.6603856, 2.8732538, 2.9527364, 2.7783957, 2.822858, 2.5790596, 2.7537866, 2.7225368, 2.6380842, 3.127516, 2.775734, 3.0380385, 2.6309524, 2.64123, 2.827846, 2.765365, 2.7560287, 2.9670591, 2.5715842, 2.712605, 2.3628695, 2.621442, 2.5037878, 2.8218498, 2.800067, 2.528994, 3.0340564, 2.7218797, 2.5950925, 2.6151245, 2.972509, 2.8951182, 2.6848931, 2.8615139, 3.009255, 2.9637263, 2.9346461, 2.660347, 2.6242654, 3.2582023, 2.9690163, 2.388488, 2.676811, 3.0589705, 2.5780191, 2.811922, 2.6962955, 2.720582, 2.680098, 2.82093, 2.7214212, 2.996432, 2.6743793, 2.9469917, 3.0759504, 2.7408998, 2.8207092, 2.9146657, 2.9011302, 3.2052138, 3.2430105, 3.012417, 3.037332, 2.8738258, 2.9063673, 2.5840497, 2.6695082, 2.814201, 2.7760568, 2.7922914, 2.7388213, 2.922627, 2.927157, 3.104983, 2.8477504, 2.629154, 2.7200947, 2.9812105, 3.3568218, 2.9329622, 3.2200966, 3.0038393, 3.1554117, 2.7952454, 2.9399877, 3.0250561, 3.0341733, 2.866405, 2.6537416, 2.8007047, 2.8833685, 2.9990852, 2.7788203, 3.2597609, 2.8269374, 2.9455192, 2.8482127, 2.6510825, 2.7316215, 2.826453, 3.0758958, 2.9076707, 2.7699668, 2.9388494, 2.8392675, 2.8179717, 2.8508534, 2.7437892, 3.160943, 2.7650676, 3.401341, 2.7385974, 2.8882923, 2.8868868, 2.773053, 2.770141, 2.7542152, 3.0286648, 2.6043942, 2.5771086, 2.7099144, 2.6209946, 2.6548443, 2.806395, 3.0500114, 2.8292534, 2.6770442, 2.886706, 2.8592734, 2.7048059, 2.8838944, 2.7610304, 3.059427, 2.770285, 2.63423, 3.0070963, 3.2303681, 2.9408505, 2.8484108, 2.72582, 2.6963112, 2.9502895, 2.923101, 2.6782105, 2.849048, 2.7265186, 2.6904519, 2.9967673, 2.968835, 2.6387184, 2.7903214, 2.9318542, 2.7282295, 2.7673314, 2.8863907, 3.0218372, 2.921461, 2.8628561, 2.7533019, 3.0752108, 2.4994822, 2.7520573, 2.694529, 2.7432554, 2.7016532, 2.9712763, 2.9286609, 2.587516, 3.0693595, 2.8585262, 2.8001242, 3.2722902, 2.8283186, 2.973534, 2.7964315, 2.8035195, 2.8833442, 2.8471122, 2.9089766, 2.8081453, 2.8030443, 2.7099319, 2.7613592, 2.772152, 3.043646, 2.7767563, 2.7614896, 2.636774, 2.7662714, 2.5834227, 2.9672143, 2.9716518, 3.0928004, 2.798427, 2.9381824, 2.620175, 2.9138613, 2.8342464, 2.7642236, 2.8616197, 2.904676, 3.0328448, 2.9008949, 2.97034, 2.5649717, 2.6884809, 2.7537527, 2.8594046, 2.9407096, 2.7397277, 3.229739, 2.8391485, 2.7015984, 2.7745621, 3.0511565, 2.6318543, 2.655181, 3.147148, 2.6860118, 2.7747855, 2.8014083, 2.8052452, 2.7719293, 2.7576697, 3.0049348, 3.0811393, 2.7912533, 2.8794122, 2.532663, 2.9530945, 2.7283733, 2.9877677, 2.909121, 2.713606, 2.9188218, 2.9776182, 2.6749413, 3.0067801, 2.9400275, 2.633561, 2.9022026, 2.8037837, 2.9123645, 2.855946, 2.9230118, 2.7139506, 2.6740015, 2.8031955, 2.9364295, 2.918338, 2.9749737, 2.9419985, 2.5620444, 2.8223326, 3.0391576, 3.0640044, 2.9441323, 2.816099, 2.8885186, 2.813156, 3.205204, 2.8781419, 2.7926373, 2.874062, 2.9949868, 2.9070945, 2.7206545, 2.7480903, 2.7885785], [4.18086, 3.220194, 2.9271548, 2.7580886, 2.7880793, 2.7742927, 3.2673423, 2.5161645, 2.6229687, 2.7129233, 2.5500412, 2.4641454, 2.6010957, 2.653257, 2.6489697, 2.7560866, 2.6892483, 2.666095, 2.6343877, 2.6160605, 2.5110588, 2.4890337, 2.5014675, 2.65289, 2.3709445, 2.3592894, 2.7384284, 2.4689307, 2.4019468, 2.3533475, 2.3579962, 2.3378937, 2.4423366, 2.4331026, 2.3804567, 2.5014844, 2.3711245, 2.3883193, 2.619699, 2.4391527, 2.4298778, 2.3385985, 2.3628142, 2.4554174, 2.5172994, 2.5826552, 2.435824, 2.3406024, 2.5486188, 2.3894677, 2.6495996, 2.8266563, 2.411463, 2.408512, 2.5788217, 2.295531, 2.3910463, 2.4581277, 2.5487797, 2.2941613, 2.3739297, 2.5502782, 2.4827034, 2.6699226, 2.6061716, 2.3566847, 2.4501379, 2.457195, 2.6860883, 2.6262078, 2.4654176, 2.4927952, 2.5051787, 2.401949, 2.594364, 2.700809, 2.4847138, 2.6977472, 2.3108346, 2.3823693, 2.3733647, 2.522541, 2.7434187, 2.7136388, 2.5217886, 2.3046386, 2.8927605, 2.4701538, 2.429208, 2.4983385, 2.513818, 2.4776468, 2.448079, 2.3438752, 2.4902973, 2.5088584, 2.450268, 2.5090227, 2.5341935, 2.466236, 2.5214806, 2.817386, 2.4966075, 2.4014697, 2.524629, 2.5264754, 2.5394275, 2.5479789, 2.428185, 2.5365636, 2.6657953, 2.561769, 2.4082377, 2.5187037, 2.4747322, 2.5343235, 2.6904538, 2.4940467, 2.585445, 2.5300138, 2.5794687, 2.4087481, 2.76061, 3.0839322, 2.371175, 2.4255533, 2.4308424, 2.4489536, 2.514844, 2.5879796, 2.588225, 2.4711182, 2.6960948, 2.7284307, 2.5900033, 2.8861663, 2.5213888, 2.541446, 2.659705, 2.5682824, 2.5420032, 2.4671082, 2.7967613, 2.457601, 2.8599472, 2.5123034, 2.6475093, 2.7677624, 2.5990987, 2.5389197, 2.6909232, 2.835275, 2.5959234, 2.7581105, 2.6454663, 2.7611177, 2.5423195, 2.6152577, 2.6894383, 2.575392, 2.5611994, 2.75755, 2.636458, 2.7557437, 2.7199512, 2.6472292, 2.8676376, 2.7880578, 2.522729, 2.8183105, 2.7242372, 2.6612983, 2.7331352, 2.661603, 2.6356978, 2.6350164, 2.727519, 2.8581681, 2.7049417, 2.7606661, 2.7620695, 2.785178, 2.5602257, 2.6735945, 2.8507442, 2.5768678, 2.6302588, 2.8350577, 2.653793, 2.852324, 2.6996355, 2.7996292, 2.7540092, 2.714112, 2.7394705, 2.805983, 2.6630805, 2.7052174, 2.6714394, 2.78957, 2.81525, 2.8976653, 2.8323965, 3.024637, 2.7165315, 2.6024225, 2.7850802, 2.7112129, 2.7214751, 2.7543118, 2.8262637, 2.7649121, 2.4438035, 2.6754332, 2.673826, 2.7001421, 2.7387595, 2.6639583, 2.8208294, 2.7259533, 2.6506371, 2.8093362, 2.6894803, 2.7111013, 2.7487612, 3.0456283, 2.6818154, 2.850065, 2.7687945, 2.7707486, 2.5845711, 2.6671767, 2.72975, 2.7064233, 2.5894992, 2.7114067, 2.7578423, 2.7350962, 2.7910662, 2.7075663, 2.7543674, 2.668576, 2.7668085, 2.7303886, 2.6281052, 2.9287317, 2.9451165, 2.8969572, 2.6750922, 2.9527214, 2.7146447, 2.6934621, 2.6262367, 2.6725516, 2.6972423, 2.7419634, 2.8449883, 2.5865853, 3.079071, 2.7600675, 2.7359831, 2.7715275, 2.684461, 2.6447241, 2.7193365, 2.6791747, 2.7498791, 2.7419448, 2.7278702, 2.7998056, 2.7911186, 2.6892927, 2.825355, 2.6924384, 2.7658918, 2.7445621, 2.8687031, 2.9010012, 2.652621, 2.674398, 2.7127426, 2.654182, 2.7233958, 2.8051355, 2.8657398, 2.9214096, 2.8567939, 2.7368345, 2.813151, 2.8944373, 2.7172887, 2.7760673, 2.7740638, 2.6855674, 2.778819, 2.8279998, 2.90547, 2.7986977, 2.8831224, 2.802904, 2.8984718, 2.7383437, 2.87862, 2.8429284, 2.7394555, 2.8800821, 2.7533488, 2.670373, 2.6547775, 2.9783533, 2.6241972, 2.8158662, 2.6670122, 2.7709162, 2.805319, 2.8272274, 2.7565567, 2.8740735, 2.8958428, 2.840772, 2.8124013, 2.698768, 2.6892843, 2.7704918, 2.840313, 2.7687385, 2.7115085, 2.6232028, 2.720048, 2.759606, 2.7400568, 2.7504487, 2.7097957, 2.7815409, 2.8023667, 2.7033784, 2.7342536, 2.8047884, 2.6420412, 2.6499426, 2.6470954, 2.7386417, 2.6060207, 2.749342, 2.580318, 2.7118893, 2.6991189, 2.7112167, 2.7260923, 2.8300862, 2.7165031, 2.5585067, 2.7188144, 2.7634954, 2.804879, 2.6274562, 2.8049715, 2.7081015, 2.6130812, 2.7062664, 2.7971506, 2.798272, 2.8820825, 2.7149463, 2.6665263, 2.730153, 2.7607353, 2.635179, 2.7196405, 2.83657, 2.6450498, 2.7530181, 2.9284925, 2.626216, 2.7300954, 2.6859128, 2.6389816, 2.7024522, 2.7626016, 2.6783059, 2.8195066, 2.6463983, 2.6842873, 2.7770529, 2.7122614, 2.7833655, 2.7044475, 2.5986083, 2.7869763, 2.8559854, 2.6658065, 2.616098, 2.6962168, 2.7228956, 2.872327, 2.7428327, 2.6417348, 2.7707896, 2.7435877, 2.7524223, 2.7942164, 2.648227, 2.6661243, 2.6754718, 2.6683664, 2.647056, 2.876538, 2.7508297, 2.768784, 2.8231418, 2.690435, 2.818818, 2.8118153, 2.7962964, 2.702848, 2.7253149, 2.734184, 2.7031884, 2.7146301, 2.739288, 2.5719817, 2.7323036, 2.7315977, 2.6713164, 2.7552762, 2.7473202, 2.6911354, 2.7094185, 2.7220144, 2.5981987, 2.845995, 2.6910064, 2.6841183, 2.7414732, 2.7238553, 2.6865072, 2.815818, 2.7707465, 2.771305, 2.8016472, 2.877779, 2.750084, 2.6973743, 2.7586281, 2.680533, 2.797619, 2.641754, 2.6528819, 2.6346686, 2.8196297, 2.7677927, 2.8308809, 2.7316418, 2.7630744, 2.8346848, 2.731018, 2.77078, 2.7673998, 2.7632198, 2.7282782, 2.783208, 2.8169005, 2.8370924, 2.8088174, 2.7779958, 2.7096531, 2.7430036, 2.8208616, 2.7294912, 2.8808582, 2.7904677, 2.8354595, 2.8929026, 2.7821286, 2.7682917, 2.801872, 2.827155, 2.7834945, 2.7987764, 2.8379529, 2.749222, 2.9422712, 2.890452, 2.8206594, 2.7704175, 2.820567, 2.8860476, 3.0045464, 2.7687676, 2.7510552, 2.877196, 2.8011076, 2.766126, 2.778656, 2.8321145, 2.7731805, 2.7710447, 2.756404, 2.7510476, 2.7328427], [6.0781317, 4.312295, 3.6565402, 3.599817, 3.0925179, 3.3838105, 2.946009, 2.9022362, 2.8408184, 2.90889, 2.756254, 2.8034863, 2.6756468, 2.529455, 2.7145863, 2.5651329, 2.758479, 2.809782, 2.7454073, 2.6461704, 2.625995, 2.422315, 2.6463966, 2.7081437, 2.620281, 2.715828, 2.5891862, 2.5537584, 2.5415263, 2.639322, 2.8104722, 2.454379, 2.559775, 2.9939995, 2.7850206, 2.375279, 2.5375094, 2.506445, 2.8080735, 2.4882073, 2.6648998, 2.4917836, 2.7755876, 2.5501838, 2.4419115, 2.4728043, 2.5613904, 2.509699, 2.4249258, 2.5209677, 2.594218, 2.5789783, 2.85037, 2.4699583, 2.440709, 2.6335394, 2.5474992, 2.4313753, 2.5424657, 2.3107743, 2.3223088, 2.6474211, 2.6403422, 2.683222, 2.3133285, 2.4384649, 2.5304089, 2.4856112, 2.51676, 2.3909547, 2.6786382, 2.5801938, 2.44455, 2.3863022, 2.498778, 2.4483182, 2.5105984, 2.3976743, 2.3638418, 2.4853225, 2.4906678, 2.4915473, 2.5196354, 2.5173206, 2.4931564, 2.5384974, 2.3866534, 2.6024325, 2.5674484, 2.4369276, 2.4466789, 2.5162895, 2.396566, 2.3464754, 2.4140837, 2.6047654, 2.40403, 2.3403747, 2.4917119, 2.5460882, 2.5212257, 2.4336305, 2.424074, 2.267264, 2.3739305, 2.5975113, 2.4786618, 2.29149, 2.5493612, 2.2801096, 2.328877, 2.388239, 2.4548044, 2.6896348, 2.6160028, 2.4799242, 2.389703, 2.5253725, 2.523201, 2.437856, 2.4189079, 2.436422, 2.3949158, 2.5081677, 2.5113156, 2.419543, 2.620697, 2.417332, 2.4083416, 2.3642209, 2.4884772, 2.4473321, 2.5451436, 2.4412525, 2.401913, 2.3585553, 2.3544228, 2.316057, 2.255808, 2.6017923, 2.343774, 2.3213995, 2.40913, 2.2333434, 2.4041677, 2.4526298, 2.5589242, 2.4217494, 2.2837894, 2.3535333, 2.2998931, 2.7547762, 2.5499356, 2.435275, 2.3785534, 2.325577, 2.4750173, 2.4234443, 2.3515577, 2.3691046, 2.5744643, 2.4127302, 2.4654758, 2.4871936, 2.4260542, 2.4443638, 2.4329796, 2.4010613, 2.3139334, 2.539993, 2.8100302, 2.429079, 2.4475524, 2.6394022, 2.5117803, 2.5875292, 2.7516298, 2.4848955, 2.7175155, 2.4999616, 2.5985923, 2.4646566, 2.6376967, 2.5967512, 2.7677383, 2.3399875, 2.418159, 2.455375, 2.417991, 2.4804883, 2.4099019, 2.4231768, 2.526698, 2.6064992, 2.4188359, 2.322302, 2.4978685, 2.5928967, 2.4062583, 2.5058117, 2.495878, 2.517448, 2.4434395, 2.5139983, 2.5136263, 2.3800313, 2.5189888, 2.3876278, 2.5048425, 2.5917044, 2.4409952, 2.537349, 2.5439298, 2.6083744, 2.4211998, 2.485649, 2.3984628, 2.529399, 2.7133698, 2.6387954, 2.4477181, 2.4927633, 2.6209855, 2.5441978, 2.4662955, 2.4689548, 2.4559736, 2.462273, 2.585961, 2.4717896, 2.4722254, 2.4842086, 2.5823734, 2.5681608, 2.6572623, 2.4856784, 3.221102, 2.5346718, 2.7783744, 2.5623834, 2.872022, 2.627671, 2.4972372, 2.472906, 2.5715363, 2.588485, 2.4431818, 2.4881942, 2.5832021, 2.6104774, 2.574462, 2.7887933, 2.6075938, 2.6681778, 2.581445, 2.5262713, 2.7602587, 2.8289084, 2.55891, 2.6048713, 2.7415078, 2.4551356, 2.9888513, 2.4875767, 2.5713677, 2.5635092, 2.6401014, 2.5998385, 2.441696, 2.5544486, 2.563933, 2.5468514, 2.5861816, 2.6651814, 2.6648788, 2.6248622, 2.7777462, 2.5451283, 2.6729908, 2.549493, 2.5025718, 2.5717273, 2.6563046, 2.7672582, 2.7730186, 2.7817473, 2.6613655, 2.6676803, 2.5876048, 2.65532, 2.5852492, 2.5723953, 2.5879366, 2.7448878, 2.5955808, 2.829625, 2.813317, 2.6107163, 2.8158512, 2.7717967, 2.5970628, 2.8018463, 2.7662978, 2.6578164, 2.6873505, 2.675254, 2.6623087, 2.752433, 2.6459906, 2.7530804, 2.6488922, 2.6658747, 2.715159, 2.9545865, 2.7210197, 2.7850451, 2.6026378, 2.673684, 2.9251337, 2.8513396, 2.7550385, 2.734495, 2.6638885, 2.6936338, 2.6100035, 2.7041984, 2.7414937, 2.6909726, 2.880304, 2.664161, 2.690406, 2.7069466, 2.731753, 2.8429704, 2.7890954, 2.992008, 2.639104, 2.6905403, 2.7696557, 2.8723202, 2.9946637, 2.748155, 2.8426378, 2.6931243, 2.7955334, 2.842241, 2.867825, 3.032939, 2.9663987, 2.7734241, 2.7834363, 2.9584713, 2.794314, 2.7702274, 2.848167, 2.819613, 2.7645984, 2.7948384, 2.796164, 3.030124, 2.9463317, 2.890264, 2.741747, 2.8496113, 2.870058, 2.700887, 2.7958047, 2.800218, 2.7172987, 2.747599, 2.721588, 2.713806, 2.763892, 2.9413083, 2.687957, 2.7418032, 2.8323903, 2.764953, 2.7584581, 2.7342868, 2.7603636, 2.6929333, 2.7228124, 2.7101376, 2.786063, 2.8956876, 2.9803727, 2.6801116, 2.780817, 2.9136949, 2.807688, 3.073508, 2.7529376, 3.2057052, 2.8341665, 2.6867585, 2.7397764, 2.758998, 2.7394428, 2.668272, 2.6984797, 2.8068528, 2.7441936, 2.9312937, 2.8068342, 2.67164, 2.96516, 2.6987202, 2.7970557, 2.7143927, 2.7748775, 2.72614, 2.8106055, 2.8936384, 2.7027252, 2.7596257, 2.8758175, 2.6829696, 2.7459593, 2.84546, 2.779221, 2.7964554, 2.876823, 2.8661227, 2.9293575, 2.8689108, 2.944907, 2.820474, 2.9199963, 2.835437, 2.8312619, 2.7961268, 2.9711573, 2.7769418, 2.950192, 2.9310384, 2.8308475, 2.8028283, 2.8429263, 3.0481458, 3.0411751, 3.0214708, 2.8953452, 2.8171904, 2.7444465, 2.696926, 2.8186133, 2.915058, 2.9015992, 2.7208197, 2.719265, 2.7013502, 2.7919579, 2.7447171, 2.750744, 2.8172665, 2.9795518, 2.7177505, 2.8188598, 2.8773115, 2.847581, 2.8104947, 2.7424765, 2.7762697, 2.7102628, 2.8084526, 2.714515, 2.7106717, 2.9477983, 2.7127745, 2.8598938, 2.7889369, 3.1115112, 2.7354033, 2.7268608, 2.8436248, 2.965603, 2.7778075, 2.9231093, 2.7435775, 2.8192582, 2.6777492, 2.8636115, 2.836019, 2.7653875, 2.7917635, 2.849734, 3.0746512, 2.8889089, 2.9674335, 2.7361178, 2.8714974, 2.8259344, 2.9236267, 2.813621, 3.0157378, 2.7599926, 2.8444102, 3.0214207, 2.9618292]]
'''
```

```python
print(all_scores)
mean = np.mean(all_scores)
stddev = np.std(all_scores)

print(u'%.2f \u00B1(%.2f)'%(mean, stddev))

'''
[2.2316887, 2.6087942, 2.6762898, 2.389614]
2.48 ±(0.18)
'''
```

```python
# 4-fold의 평균 validation MAE 그래프 그리기

print(model.metrics_names)

'''
['loss', 'mean_absolute_error']
'''
```



근데 이거는 epoch 가 100일때의 가정이라서 잘 뽑힌거고,
일부러 overfitting 되는 상황을 만들어서 한번 출력해보자.

내 생각엔 epoch 를 500까지 안둬도 될 것 같고, 지금 100때의 MAE 값들 나오는거 보면 대충 250 정도만 해도 충분히 Overfitting 될 것 같음... (강사님 의견도 동일해서, 예제랑 다르게 그냥 epoch 를 250으로 돌려봄)



```python
k = 4
num_val_samples = len(train_data) // k
num_epochs = 250
all_mae_histories = []

for i in range(k):
    print('>> fold ', i)
    val_data = train_data[i * num_val_samples: (i+1) * num_val_samples]
    val_targets = train_targets[i * num_val_samples: (i+1) * num_val_samples]
    
    partial_train_data = np.concatenate([train_data[:i*num_val_samples],
                                         train_data[(i+1) * num_val_samples:]],
                                        axis=0)
    partial_train_targets = np.concatenate([train_targets[:i*num_val_samples],
                                            train_targets[(i+1) * num_val_samples:]],
                                           axis=0)
    
    model = build_model()
    hist = model.fit(partial_train_data, 
                     partial_train_targets,
                     epochs = num_epochs,
                     batch_size = 1,
                     verbose = 0,
                     validation_data = (val_data, val_targets))
    
    mae_history = hist.history['val_mean_absolute_error']
    all_mae_histories.append(mae_history)
    
print('finished!!')
```

학습이 끝나면, 과적합 상황을 시각화해서 한번 뽑아보자.
(mae 값들을 print 하는건 너무 많고 내가 귀찮으니 걍 생략함)


```python
average_mae_history = [
    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]
```

```python
average_mae_history = []
for epoch in range(num_epochs):
    average_mae_per_epoch = []
    
    for mae_history in all_mae_histories: # 4개 fold 반복
        average_mae_per_epoch.append(mae_history[epoch])
    
    mean = np.mean(average_mae_per_epoch)
    average_mae_history.append(mean) 
```    

둘이 같은 표현이다.

그래서 시각화를 시키면,

```python
import matplotlib.pyplot as plt
%matplotlib inline

plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)
plt.xlabel('Epochs')
plt.ylabel('Validation MAE')
plt.show()
```

(실행결과는 여기를 참고하자)
[강사님 깃](https://github.com/tjrjsgk/temp/blob/master/6_regression(boston%20housing).ipynb)


(강사님 깃은 epoch 500 기준이니깐 그래프가 나랑 좀 다르게 나타날것)

그니깐... Epoch 가 일정수준이 넘어가면.... Overfit 된걸 볼 수 있음

여기에 아까 위에서 언급한 지수 이동 평균 필터링을 적용해본다.



#### 지수 이동 평균 (EMA, Exponential Moving Averge)



간단하게 다시 짚으면, 평균을 구할때 시간이라는 개념이 추가됨.

지수 이동 평균은, 최근에 높은 가중치를 주지만 *오래된 과거도 비록 낮은 영향력이지만 가중치를 두여하도록* 고려한 방법.

이제 코드에 적용해보자.



```python
'''
시계열 신호에 지수 이동 평균(exponential moving averge) 필터링
'''
def smooth_curve(points, factor=0.9):
    smoothed_points = []
    for point in points:
        if smoothed_points:
            prev = smoothed_points[-1]
            smoothed_points.append(prev*factor + point*(1-factor))
        else:
            smoothed_points.append(point)
    
    return smoothed_points

smooth_mae_history = smooth_curve(average_mae_history[10:])

plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)
plt.xlabel('Epochs')
plt.ylabel('Validation MAE')
plt.show()
```


이걸 적용시키면 아까랑 다른 플롯 와꾸가 나옴.
(좀 더 그럴듯하고 간지나보임)



**5. (과대적합 되기 전까지) 모델 학습**


새로운 모델 객체를 생성해서 과적합 전까지 재학습을 시켜봅시다....

plot 뽑아본거 보면, epoch 가 대충 눈대중으로 80정도면 과적합 ㄴㄴ


```python
# 기존 model 객체를 사용하면 이어서 학습되므로, 새로운 model 객체를 생성함
model = build_model()
model.fit(train_data, 
          train_targets,
          epochs = 80,
          batch_size = 16,
          verbose = 1)
```

```python
'''
Epoch 1/80
404/404 [==============================] - 0s 654us/sample - loss: 498.5625 - mean_absolute_error: 20.3979
Epoch 2/80
404/404 [==============================] - 0s 252us/sample - loss: 338.1196 - mean_absolute_error: 16.1996
Epoch 3/80
404/404 [==============================] - 0s 240us/sample - loss: 170.0173 - mean_absolute_error: 10.6090
Epoch 4/80
404/404 [==============================] - 0s 239us/sample - loss: 71.5559 - mean_absolute_error: 6.3844
Epoch 5/80
404/404 [==============================] - 0s 238us/sample - loss: 42.0920 - mean_absolute_error: 4.7063
  ...
Epoch 76/80
404/404 [==============================] - 0s 190us/sample - loss: 6.1969 - mean_absolute_error: 1.7329
Epoch 77/80
404/404 [==============================] - 0s 195us/sample - loss: 6.1915 - mean_absolute_error: 1.6974
Epoch 78/80
404/404 [==============================] - 0s 197us/sample - loss: 6.0633 - mean_absolute_error: 1.6931
Epoch 79/80
404/404 [==============================] - 0s 195us/sample - loss: 5.8208 - mean_absolute_error: 1.6877
Epoch 80/80
404/404 [==============================] - 0s 195us/sample - loss: 5.9284 - mean_absolute_error: 1.6594

<tensorflow.python.keras.callbacks.History at 0x23e79f51eb8>  
'''
```

loss: 5.9284 - mean_absolute_error: 1.6594

이 정도면 대충 성공적인듯.



**6. 테스트셋으로 성능 평가하기**



```python
test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)
print('test MSE score: %.3f', test_mse_score)
print('test MAE score: %.3f', test_mae_score)
```

```python
'''

102/102 [==============================] - 0s 978us/sample - loss: 17.6753 - mean_absolute_error: 2.7005
test MSE score: %.3f 17.675254821777344
test MAE score: %.3f 2.7004523
'''
```


MSE는 대충 17.6, MAE는 대충 2.7 정도 나온다.

이 정도면 좋은 예측이라고 볼 수 있음.


> 이런 식으로, MAE 값을 계속 비교해 나가면서 k 값도 적당하게 쪼개보고, 적합한 수준의 성능평가 수준을 찾으면 된다.
